<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>VAE on Sihan`s Blog</title>
    <link>https://sihan-son.github.io/tags/vae/</link>
    <description>Recent content in VAE on Sihan`s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ko-kr</language>
    <lastBuildDate>Wed, 03 Jul 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://sihan-son.github.io/tags/vae/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>간단하게 알아보는 Difference between VAE and GAN</title>
      <link>https://sihan-son.github.io/mldl/difference-between-vae-and-gan/</link>
      <pubDate>Wed, 03 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://sihan-son.github.io/mldl/difference-between-vae-and-gan/</guid>
      <description>What is difference between VAE and GAN VAE와 GAN은 그림에서 보다시피 Maximum Likehood의 범주에 속하는 방법론이다. 그림에서 볼 수 있든 Explicit한 방법론과 Implicit한 방법론으로 나뉜다. 이 블로그에서 주로 다루는 GAN은 보다시피 Implicit(암시적인)한 방법론을 취하고 있다.
VAE Variational Autoencoder(AVE)는 Kingma et al1의 논문에서 제안된 네트워크의 구조이다. 복잡한 데이터 생성 모델을 설계하고 대규모 데이터 세트에 적용을 할 수 있게 해준다. input을 z로 encoding하고 스스로 input을 decoding하는 방법을 학습하는 방법이다. 즉 decoding된 output이 input과 최대한 가깝게 만들어는 내는 방법이다.</description>
    </item>
    
  </channel>
</rss>
