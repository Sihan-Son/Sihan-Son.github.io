<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ML/DL on Sihan`s Blog</title>
    <link>https://sihan-son.github.io/categories/ml/dl/</link>
    <description>Recent content in ML/DL on Sihan`s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ko-kr</language><atom:link href="https://sihan-son.github.io/categories/ml/dl/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>간단하게 훑어보는 Advanced CNNs</title>
      <link>https://sihan-son.github.io/mldl/advanced-cnn/</link>
      <pubDate>Tue, 24 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://sihan-son.github.io/mldl/advanced-cnn/</guid>
      <description>언제나 처럼 다시 돌아온 간단하게 Deep Learning 개념 정리 글입니다. 자세한 내용은 각 논문을 참조하거나 다른 학술 블로그를 읽어 주세요. 이 글에서 언급 되는 아키텍쳐들에 관한 논문 제목은 글 하단의 각주로 확인해 주세요.
Index 1. AlexNet
2. VGGNet, GoogLeNet
3. ResNet
fig.1 ILSVRC 우승 네트워크 레이어 수
2012년 AlexNet을 시작으로 Convolution Neural Network CNN은 빠른 속도로 성능이 좋아지고 위 이미지에서 보이는 것 처럼 레이어가 깊어지기 시작했습니다. 이후 모델들은 가중치 업데이트를 위해 사용한 Error Backpropagation 알고리즘의 단점인 Gradient Vanishing과 Gradient Exploding 문제를 극복하고 나온 모델들입니다.</description>
    </item>
    
    <item>
      <title>간단하게 정리한 Mode Collapse</title>
      <link>https://sihan-son.github.io/mldl/mode-collapse/</link>
      <pubDate>Tue, 16 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://sihan-son.github.io/mldl/mode-collapse/</guid>
      <description>저는 언제나 처럼 간단하게 이게 어떤 개념인지만 짚고 넘어가겠습니다. 자세한 내용은 다른 학술 블로그들을 참조해 주세요!
GAN관련 논문이나 자료들을 읽다 보면은 심심치 않게 mode collapse라는 말을 발견 할 수 있습니다. 여기서 mode는 수학에서 말하는 최빈값입니다. 즉 제일 자주 등장하는 값들을 말합니다. mode collapse는 보통 Multi-Modal일 경우 두드러지게 발생 할 수 있습니다. 튜토리얼로 자주 사용하는 MNIST의 경우 &amp;lsquo;0~9&amp;rsquo; 10개의 mode를 갖게 됩니다.
Generator G가 input z를 하나의 mode에 치우쳐 변화시키는 현상이 발생합니다.</description>
    </item>
    
    <item>
      <title>간단하게 정리한 Norm</title>
      <link>https://sihan-son.github.io/mldl/what-is-norm/</link>
      <pubDate>Tue, 16 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://sihan-son.github.io/mldl/what-is-norm/</guid>
      <description>저는 언제나 처럼 간단하게 이게 어떤 개념인지만 짚고 넘어가겠습니다. 자세한 내용은 다른 학술 블로그들을 참조해 주세요! 그럼 이번 글에서는 Norm에 대한 개념을 간단하게 잡아 봅시다!
What is Norm? Norm은 수학적으로 벡터 공간 또는 행렬에 있는 모든 벡터의 전체 크기, 길이를 의미합니다. 단순화를 위해 표준이 높을수록 행렬 또는 벡터의 값이 커집니다.
p: Norm의 차수(p의 차수에 따라 L0, L1, L2 결정)
N: 대상 벡터의 요소 수
L0 Norm 실제로 Norm은 아닙니다. 벡터의 0이 아닌 요소의 총 개수를 의미합니다.</description>
    </item>
    
    <item>
      <title>간단하게 알아보는 Difference between VAE and GAN</title>
      <link>https://sihan-son.github.io/mldl/difference-between-vae-and-gan/</link>
      <pubDate>Wed, 03 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://sihan-son.github.io/mldl/difference-between-vae-and-gan/</guid>
      <description>What is difference between VAE and GAN VAE와 GAN은 그림에서 보다시피 Maximum Likehood의 범주에 속하는 방법론이다. 그림에서 볼 수 있든 Explicit한 방법론과 Implicit한 방법론으로 나뉜다. 이 블로그에서 주로 다루는 GAN은 보다시피 Implicit(암시적인)한 방법론을 취하고 있다.
VAE Variational Autoencoder(AVE)는 Kingma et al1의 논문에서 제안된 네트워크의 구조이다. 복잡한 데이터 생성 모델을 설계하고 대규모 데이터 세트에 적용을 할 수 있게 해준다. input을 z로 encoding하고 스스로 input을 decoding하는 방법을 학습하는 방법이다. 즉 decoding된 output이 input과 최대한 가깝게 만들어는 내는 방법이다.</description>
    </item>
    
    <item>
      <title>Simple Latent Space</title>
      <link>https://sihan-son.github.io/mldl/latent-space/</link>
      <pubDate>Mon, 01 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://sihan-son.github.io/mldl/latent-space/</guid>
      <description>오늘은 Generative adversarial network GAN 논문을 읽거나 자료를 접하다보면 자주 보는 latent space에 대한 자료를 포스팅 하려고 합니다. 이 포스트는 아마도 지속적으로 업데이트가 진행되면서 내용이 풍부해지길 저도 기대하고 있습니다. 이 포스팅은 latent space에 대해서 간단하게 알아보는 포스팅이니 개념만 잡고 가세요. 혹시 잘못된 내용이 있다면 메일이나 댓글로 피드백 주시면 수정하도록 하겠습니다!
머신러닝의 성능은 데이터의 양과 질에 굉장히 의존적입니다. Trash in Trash out 말이 있듯이 데이터의 질에 성능이 심히 요동치게 됩니다.
그래서 데이터가 모이면 어떤 feature가 유용한지 아닌지 확인하는 작업이 필요로 합니다.</description>
    </item>
    
  </channel>
</rss>
